{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09ce8a9",
   "metadata": {},
   "source": [
    "# Which Type of Student Are You? - ML Classification Project\n",
    "## End-Semester Machine Learning Lab Project\n",
    "\n",
    "**Objective:** Predict student category (Topper, Backbencher, Crammer, All-Rounder) using supervised and unsupervised ML algorithms.\n",
    "\n",
    "**Dataset Features:**\n",
    "- study_hours: Hours spent studying per day\n",
    "- attendance: Attendance percentage\n",
    "- assignments: Whether assignments are completed (1=Yes, 0=No)\n",
    "- social_media: Hours spent on social media per day\n",
    "- sleep_hours: Hours of sleep per day\n",
    "- backlogs: Whether student has backlogs (1=Yes, 0=No)\n",
    "- student_type: Target variable (Topper/Backbencher/Crammer/All-Rounder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d8353",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Supervised Learning Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Unsupervised Learning\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Association Rule Mining\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Model Persistence\n",
    "import pickle\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f02da",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ef4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/student_type_dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\\n\")\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Total Records: {df.shape[0]}\")\n",
    "print(f\"Total Features: {df.shape[1]}\\n\")\n",
    "\n",
    "# Display first few records\n",
    "print(\"First 5 records:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ec732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540920fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal Missing Values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b9307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of target variable\n",
    "print(\"Distribution of Student Types:\")\n",
    "print(df['student_type'].value_counts())\n",
    "\n",
    "# Visualize distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['student_type'].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c', '#f39c12', '#3498db'])\n",
    "plt.title('Distribution of Student Types', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Student Type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49aba28",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "**Why preprocessing is important:**\n",
    "- Label encoding converts categorical target variable into numerical format\n",
    "- Train-test split ensures model evaluation on unseen data\n",
    "- Prevents overfitting and gives realistic accuracy estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('student_type', axis=1)\n",
    "y = df['student_type']\n",
    "\n",
    "print(\"Features (X):\")\n",
    "print(X.head())\n",
    "print(f\"\\nFeature Shape: {X.shape}\")\n",
    "\n",
    "print(\"\\nTarget (y):\")\n",
    "print(y.head())\n",
    "print(f\"Target Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb48a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for target variable\n",
    "# Converts: Topper, Backbencher, Crammer, All-Rounder -> 0, 1, 2, 3\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(\"Original Labels:\")\n",
    "print(label_encoder.classes_)\n",
    "print(\"\\nEncoded Labels:\")\n",
    "print(np.unique(y_encoded))\n",
    "print(\"\\nMapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf19c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split (80% training, 20% testing)\n",
    "# random_state=42 ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "print(f\"Training Set Size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing Set Size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining Set: {X_train.shape[0]/df.shape[0]*100:.1f}%\")\n",
    "print(f\"Testing Set: {X_test.shape[0]/df.shape[0]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fded4d",
   "metadata": {},
   "source": [
    "## Step 4: Supervised Learning Models\n",
    "\n",
    "**Supervised Learning:** Learning from labeled data where the target (student_type) is known.\n",
    "\n",
    "We will train and evaluate 5 different classification algorithms:\n",
    "1. **Decision Tree:** Makes decisions based on feature conditions\n",
    "2. **Naive Bayes:** Based on probability and Bayes theorem\n",
    "3. **K-Nearest Neighbors:** Classifies based on nearest data points\n",
    "4. **Support Vector Machine:** Finds optimal decision boundary\n",
    "5. **Random Forest:** Ensemble of multiple decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d3680",
   "metadata": {},
   "source": [
    "### 4.1 Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "# Creates a tree-like model of decisions based on features\n",
    "# Each node represents a feature, branches represent decisions\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, dt_predictions, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546854e4",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aec6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "# Based on Bayes Theorem: P(A|B) = P(B|A) * P(A) / P(B)\n",
    "# Assumes features are independent (naive assumption)\n",
    "# Very fast and works well with small datasets\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, nb_predictions, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2d06d",
   "metadata": {},
   "source": [
    "### 4.3 K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745137e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "# Classifies based on majority vote of K nearest neighbors\n",
    "# K=5 means it looks at 5 closest data points\n",
    "# Distance-based algorithm (Euclidean distance)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "knn_predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "print(f\"K-Nearest Neighbors Accuracy: {knn_accuracy*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, knn_predictions, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c21366",
   "metadata": {},
   "source": [
    "### 4.4 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Classifier\n",
    "# Finds the optimal hyperplane that separates different classes\n",
    "# Maximizes the margin between classes\n",
    "# RBF kernel handles non-linear relationships\n",
    "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(f\"Support Vector Machine Accuracy: {svm_accuracy*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, svm_predictions, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a11138",
   "metadata": {},
   "source": [
    "### 4.5 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "# Ensemble method: Combines multiple decision trees\n",
    "# Each tree votes for a class, majority wins\n",
    "# n_estimators=100 means 100 trees in the forest\n",
    "# Reduces overfitting and increases accuracy\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy*100:.2f}%\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_predictions, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d297210",
   "metadata": {},
   "source": [
    "### Comparison of All Supervised Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Algorithm': ['Decision Tree', 'Naive Bayes', 'K-Nearest Neighbors', 'Support Vector Machine', 'Random Forest'],\n",
    "    'Accuracy': [dt_accuracy*100, nb_accuracy*100, knn_accuracy*100, svm_accuracy*100, rf_accuracy*100]\n",
    "})\n",
    "\n",
    "results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON - ACCURACY RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(results['Algorithm'], results['Accuracy'], \n",
    "               color=['#2ecc71', '#3498db', '#f39c12', '#e74c3c', '#9b59b6'])\n",
    "plt.title('Supervised Learning Models - Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Algorithm', fontsize=12)\n",
    "plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "plt.ylim(0, 110)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}%',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best performing model\n",
    "best_model_name = results.iloc[0]['Algorithm']\n",
    "best_accuracy = results.iloc[0]['Accuracy']\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model_name}\")\n",
    "print(f\"üéØ Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c53f34",
   "metadata": {},
   "source": [
    "## Step 5: Unsupervised Learning\n",
    "\n",
    "**Unsupervised Learning:** Learning patterns from unlabeled data without predefined categories.\n",
    "\n",
    "**Why use it?**\n",
    "- Discover hidden patterns in data\n",
    "- Group similar students together without labels\n",
    "- Validate if natural groupings match our labeled categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a433d4",
   "metadata": {},
   "source": [
    "### 5.1 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaf625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering\n",
    "# Partitions data into K clusters based on similarity\n",
    "# Each cluster has a centroid (center point)\n",
    "# Points are assigned to nearest centroid\n",
    "# We use 4 clusters matching our 4 student types\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "kmeans_clusters = kmeans.fit_predict(X)\n",
    "\n",
    "print(\"K-Means Clustering Results:\")\n",
    "print(f\"Number of clusters: 4\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "unique, counts = np.unique(kmeans_clusters, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"Cluster {cluster}: {count} students\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df_clustered = df.copy()\n",
    "df_clustered['KMeans_Cluster'] = kmeans_clusters\n",
    "\n",
    "print(\"\\nSample of clustered data:\")\n",
    "print(df_clustered.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e6706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Means clustering (using first 2 features)\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df['study_hours'], df['attendance'], \n",
    "                     c=kmeans_clusters, cmap='viridis', s=100, alpha=0.6, edgecolors='black')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "           c='red', s=300, marker='X', edgecolors='black', linewidths=2, label='Centroids')\n",
    "plt.title('K-Means Clustering: Study Hours vs Attendance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Study Hours', fontsize=12)\n",
    "plt.ylabel('Attendance (%)', fontsize=12)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411960e",
   "metadata": {},
   "source": [
    "### 5.2 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2cdf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering (Agglomerative)\n",
    "# Bottom-up approach: Each point starts as its own cluster\n",
    "# Gradually merges closest clusters until K clusters remain\n",
    "# Creates a dendrogram (tree structure) showing relationships\n",
    "\n",
    "hierarchical = AgglomerativeClustering(n_clusters=4)\n",
    "hierarchical_clusters = hierarchical.fit_predict(X)\n",
    "\n",
    "print(\"Hierarchical Clustering Results:\")\n",
    "print(f\"Number of clusters: 4\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "unique, counts = np.unique(hierarchical_clusters, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"Cluster {cluster}: {count} students\")\n",
    "\n",
    "# Add to dataframe\n",
    "df_clustered['Hierarchical_Cluster'] = hierarchical_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dendrogram (hierarchical tree)\n",
    "plt.figure(figsize=(15, 8))\n",
    "linkage_matrix = linkage(X.iloc[:50], method='ward')  # Using first 50 samples for clarity\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=20)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Sample)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Sample Index or Cluster Size', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b49bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Hierarchical clustering\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(df['study_hours'], df['attendance'], \n",
    "                     c=hierarchical_clusters, cmap='plasma', s=100, alpha=0.6, edgecolors='black')\n",
    "plt.title('Hierarchical Clustering: Study Hours vs Attendance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Study Hours', fontsize=12)\n",
    "plt.ylabel('Attendance (%)', fontsize=12)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a6349",
   "metadata": {},
   "source": [
    "## Step 6: Feature Selection using PCA\n",
    "\n",
    "**PCA (Principal Component Analysis):**\n",
    "- Reduces number of features while preserving important information\n",
    "- Transforms correlated features into uncorrelated principal components\n",
    "- First component captures maximum variance, second captures next most, etc.\n",
    "- Useful for visualization and reducing computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensions to 2\n",
    "# This allows us to visualize high-dimensional data in 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(f\"Original dimensions: {X.shape[1]}\")\n",
    "print(f\"Reduced dimensions: {X_pca.shape[1]}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]*100:.2f}%\")\n",
    "print(f\"PC2: {pca.explained_variance_ratio_[1]*100:.2f}%\")\n",
    "print(f\"Total variance preserved: {sum(pca.explained_variance_ratio_)*100:.2f}%\")\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(f\"By using only 2 components, we preserve {sum(pca.explained_variance_ratio_)*100:.2f}% of the original information.\")\n",
    "print(\"This means we can visualize the data in 2D while losing minimal information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA-transformed data with actual student types\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create color map for student types\n",
    "colors = {'Topper': '#2ecc71', 'Backbencher': '#e74c3c', 'Crammer': '#f39c12', 'All-Rounder': '#3498db'}\n",
    "\n",
    "for student_type in df['student_type'].unique():\n",
    "    mask = df['student_type'] == student_type\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               label=student_type, s=100, alpha=0.6, \n",
    "               edgecolors='black', c=colors[student_type])\n",
    "\n",
    "plt.title('PCA Visualization: Student Types in 2D Space', fontsize=16, fontweight='bold')\n",
    "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
    "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
    "plt.legend(loc='best', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d7779",
   "metadata": {},
   "source": [
    "## Step 7: Association Rule Mining (Apriori Algorithm)\n",
    "\n",
    "**Association Rule Mining:**\n",
    "- Discovers interesting relationships between features\n",
    "- Format: If X then Y (e.g., If low study hours THEN backbencher)\n",
    "- **Support:** How frequently items appear together\n",
    "- **Confidence:** How often the rule is true\n",
    "- **Lift:** How much more likely Y is when X occurs\n",
    "\n",
    "**Use case:** Understanding patterns like:\n",
    "- High social media + low attendance ‚Üí Backbencher\n",
    "- High study hours + high attendance ‚Üí Topper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for association rule mining\n",
    "# Convert numerical features to categorical (High/Low)\n",
    "df_apriori = df.copy()\n",
    "\n",
    "# Categorize features\n",
    "df_apriori['study_hours_cat'] = df_apriori['study_hours'].apply(lambda x: 'High_Study' if x >= 6 else 'Low_Study')\n",
    "df_apriori['attendance_cat'] = df_apriori['attendance'].apply(lambda x: 'High_Attendance' if x >= 75 else 'Low_Attendance')\n",
    "df_apriori['social_media_cat'] = df_apriori['social_media'].apply(lambda x: 'High_SocialMedia' if x >= 5 else 'Low_SocialMedia')\n",
    "df_apriori['assignments_cat'] = df_apriori['assignments'].apply(lambda x: 'Does_Assignments' if x == 1 else 'No_Assignments')\n",
    "df_apriori['backlogs_cat'] = df_apriori['backlogs'].apply(lambda x: 'Has_Backlogs' if x == 1 else 'No_Backlogs')\n",
    "\n",
    "# Create transactions (each row is a transaction)\n",
    "transactions = []\n",
    "for idx, row in df_apriori.iterrows():\n",
    "    transaction = [\n",
    "        row['study_hours_cat'],\n",
    "        row['attendance_cat'],\n",
    "        row['social_media_cat'],\n",
    "        row['assignments_cat'],\n",
    "        row['backlogs_cat'],\n",
    "        row['student_type']\n",
    "    ]\n",
    "    transactions.append(transaction)\n",
    "\n",
    "print(\"Sample transactions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Transaction {i+1}: {transactions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d257c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to one-hot encoded format for Apriori\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "print(\"Encoded data shape:\", df_encoded.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fb600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Apriori algorithm\n",
    "# min_support=0.2 means pattern must appear in at least 20% of transactions\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.2, use_colnames=True)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules\n",
    "# metric='lift' and min_threshold=1.2 ensures meaningful rules\n",
    "# Lift > 1 means items are positively correlated\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "rules = rules.sort_values('confidence', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ASSOCIATION RULES - TOP 10 PATTERNS\")\n",
    "print(\"=\"*100)\n",
    "print(\"Format: IF [antecedents] THEN [consequents]\")\n",
    "print(\"Support: Frequency of pattern | Confidence: Reliability | Lift: Strength of association\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for idx, row in rules.head(10).iterrows():\n",
    "    antecedents = ', '.join(list(row['antecedents']))\n",
    "    consequents = ', '.join(list(row['consequents']))\n",
    "    print(f\"\\nRule {idx + 1}:\")\n",
    "    print(f\"  IF {antecedents}\")\n",
    "    print(f\"  THEN {consequents}\")\n",
    "    print(f\"  Support: {row['support']:.3f} | Confidence: {row['confidence']:.3f} | Lift: {row['lift']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d76bba",
   "metadata": {},
   "source": [
    "## Step 8: Save the Best Model for Deployment\n",
    "\n",
    "**Why save the model?**\n",
    "- Avoid retraining every time we need predictions\n",
    "- Deploy model in production (web app, mobile app)\n",
    "- Share model with others without sharing training data\n",
    "- Pickle serializes the trained model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on accuracy\n",
    "accuracies = {\n",
    "    'Decision Tree': dt_accuracy,\n",
    "    'Naive Bayes': nb_accuracy,\n",
    "    'K-Nearest Neighbors': knn_accuracy,\n",
    "    'Support Vector Machine': svm_accuracy,\n",
    "    'Random Forest': rf_accuracy\n",
    "}\n",
    "\n",
    "best_model_name = max(accuracies, key=accuracies.get)\n",
    "best_accuracy = accuracies[best_model_name]\n",
    "\n",
    "# Select the best model\n",
    "model_map = {\n",
    "    'Decision Tree': dt_classifier,\n",
    "    'Naive Bayes': nb_classifier,\n",
    "    'K-Nearest Neighbors': knn_classifier,\n",
    "    'Support Vector Machine': svm_classifier,\n",
    "    'Random Forest': rf_classifier\n",
    "}\n",
    "\n",
    "best_model = model_map[best_model_name]\n",
    "\n",
    "print(f\"Best Model Selected: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and label encoder\n",
    "with open('model/student_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "\n",
    "with open('model/label_encoder.pkl', 'wb') as encoder_file:\n",
    "    pickle.dump(label_encoder, encoder_file)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(\"üìÅ Location: model/student_model.pkl\")\n",
    "print(\"üìÅ Label Encoder: model/label_encoder.pkl\")\n",
    "print(\"\\nThese files will be used by the Flask web application for predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c06013d",
   "metadata": {},
   "source": [
    "## Step 9: Test Prediction on Sample Data\n",
    "\n",
    "**Testing before deployment ensures:**\n",
    "- Model works correctly on new data\n",
    "- Input format is correct\n",
    "- Output is interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cd6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample student data\n",
    "sample_students = [\n",
    "    [8, 95, 1, 2, 7, 0],  # Expected: Topper\n",
    "    [2, 45, 0, 8, 5, 1],  # Expected: Backbencher\n",
    "    [1, 70, 0, 5, 4, 0],  # Expected: Crammer\n",
    "    [6, 88, 1, 3, 6, 0],  # Expected: All-Rounder\n",
    "]\n",
    "\n",
    "print(\"Testing Model with Sample Students:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, student in enumerate(sample_students, 1):\n",
    "    prediction = best_model.predict([student])[0]\n",
    "    student_type = label_encoder.inverse_transform([prediction])[0]\n",
    "    \n",
    "    print(f\"\\nStudent {i}:\")\n",
    "    print(f\"  Study Hours: {student[0]}, Attendance: {student[1]}%, Assignments: {'Yes' if student[2] else 'No'}\")\n",
    "    print(f\"  Social Media: {student[3]}hrs, Sleep: {student[4]}hrs, Backlogs: {'Yes' if student[5] else 'No'}\")\n",
    "    print(f\"  üéØ Predicted Type: {student_type}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Model is working correctly! Ready for deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e00a36",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Loaded and explored dataset (150 student records)\n",
    "   - Encoded target variable\n",
    "   - Split data into training (80%) and testing (20%)\n",
    "\n",
    "2. **Supervised Learning (5 algorithms):**\n",
    "   - Decision Tree Classifier\n",
    "   - Naive Bayes Classifier\n",
    "   - K-Nearest Neighbors\n",
    "   - Support Vector Machine\n",
    "   - Random Forest Classifier\n",
    "   \n",
    "3. **Unsupervised Learning:**\n",
    "   - K-Means Clustering (4 clusters)\n",
    "   - Hierarchical Clustering (Agglomerative)\n",
    "   \n",
    "4. **Feature Selection:**\n",
    "   - Applied PCA to reduce dimensions from 6 to 2\n",
    "   - Visualized data in 2D space\n",
    "   \n",
    "5. **Association Rule Mining:**\n",
    "   - Used Apriori algorithm to discover patterns\n",
    "   - Generated rules like: Low study hours ‚Üí Backbencher\n",
    "   \n",
    "6. **Model Deployment:**\n",
    "   - Saved best performing model as pickle file\n",
    "   - Ready for Flask web application\n",
    "\n",
    "### Next Steps:\n",
    "- Deploy model using Flask backend\n",
    "- Create HTML/CSS frontend for user interaction\n",
    "- Test the web application\n",
    "\n",
    "---\n",
    "\n",
    "**Project Status: ‚úÖ Training Complete | Ready for Deployment**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
